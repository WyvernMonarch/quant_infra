{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f19e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import pandas as pd\n",
    "import time  # Для задержек, если лимиты API\n",
    "\n",
    "def fetch_ohlcv(symbol, timeframe, since, limit=1000):\n",
    "    \"\"\"\n",
    "    Функция для загрузки OHLCV-данных с Binance через CCXT.\n",
    "    \n",
    "    :param symbol: Символ токена, например 'BTC/USDT'\n",
    "    :param timeframe: Таймфрейм, например '1m', '5m', '1h', '1d'\n",
    "    :param since: Начальная дата в формате ISO8601, например '2025-05-01T00:00:00Z'\n",
    "    :param limit: Максимальное число свечей на запрос (по умолчанию 1000)\n",
    "    :return: DataFrame с колонками: timestamp, open, high, low, close, volume\n",
    "    \"\"\"\n",
    "    exchange = ccxt.binance()  # Можно заменить на ccxt.kucoin() или другую\n",
    "    since_ms = exchange.parse8601(since)  # Конвертируем в миллисекунды\n",
    "    \n",
    "    all_ohlcv = []  # Список для всех свечей\n",
    "    while True:\n",
    "        try:\n",
    "            ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since_ms, limit)\n",
    "            if not ohlcv:\n",
    "                break  # Нет больше данных\n",
    "            all_ohlcv.extend(ohlcv)\n",
    "            since_ms = ohlcv[-1][0] + 1  # Следующий запрос с последней свечи +1 мс\n",
    "            print(f\"Загружено {len(ohlcv)} свечей, продолжаем...\")\n",
    "            time.sleep(1)  # Задержка для избежания rate-limit\n",
    "        except ccxt.NetworkError as e:\n",
    "            print(f\"Ошибка сети: {e}. Повтор через 5 сек...\")\n",
    "            time.sleep(5)\n",
    "        except ccxt.ExchangeError as e:\n",
    "            print(f\"Ошибка биржи: {e}\")\n",
    "            break\n",
    "    \n",
    "    if all_ohlcv:\n",
    "        df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')  # Конвертируем в читаемую дату\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Нет данных для этого периода.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Пример использования\n",
    "symbol = 'DOGE/USDT'#'BNB/USDT'#'ETH/USDT'#'SOL/USDT'#'BTC/USDT'\n",
    "timeframes = ['1d', '1h', '1m']  # Разные таймфреймы для теста\n",
    "since = '2025-05-01T00:00:00Z'  # Начало периода (май 2025)\n",
    "\n",
    "for tf in timeframes:\n",
    "    df = fetch_ohlcv(symbol, tf, since)\n",
    "    if not df.empty:\n",
    "        print(f\"\\nДанные для {tf}:\")\n",
    "        print(df.tail(5))  # Последние 5 строк\n",
    "        df.to_csv(f'doge_usdt_{tf}.csv', index=False)  # Сохраняем в CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a407a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dune_client.client import DuneClient\n",
    "dune = DuneClient(\"FRj6Bysj3Cs0Uy0WTihbWG43oDLu2CLH\")\n",
    "query_result = dune.get_latest_result(41840)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59eac5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Receiving Addresses (R)</th>\n",
       "      <th># Sending Addresses (S)</th>\n",
       "      <th># Total Addresses</th>\n",
       "      <th>Baseline</th>\n",
       "      <th>Day</th>\n",
       "      <th>S/R Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>273018</td>\n",
       "      <td>543235</td>\n",
       "      <td>626034</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-08-04 00:00:00.000 UTC</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>243543</td>\n",
       "      <td>483382</td>\n",
       "      <td>561709</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-08-03 00:00:00.000 UTC</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>278274</td>\n",
       "      <td>529392</td>\n",
       "      <td>610919</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-08-02 00:00:00.000 UTC</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300429</td>\n",
       "      <td>572052</td>\n",
       "      <td>667830</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-08-01 00:00:00.000 UTC</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>365144</td>\n",
       "      <td>697324</td>\n",
       "      <td>804438</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-07-31 00:00:00.000 UTC</td>\n",
       "      <td>1.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>138660</td>\n",
       "      <td>205602</td>\n",
       "      <td>259781</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-17 00:00:00.000 UTC</td>\n",
       "      <td>1.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>129013</td>\n",
       "      <td>192740</td>\n",
       "      <td>243738</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-16 00:00:00.000 UTC</td>\n",
       "      <td>1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>142740</td>\n",
       "      <td>204520</td>\n",
       "      <td>260385</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-15 00:00:00.000 UTC</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>150693</td>\n",
       "      <td>213564</td>\n",
       "      <td>271864</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-14 00:00:00.000 UTC</td>\n",
       "      <td>1.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>170537</td>\n",
       "      <td>233139</td>\n",
       "      <td>292884</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-13 00:00:00.000 UTC</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      # Receiving Addresses (R)  # Sending Addresses (S)  # Total Addresses  \\\n",
       "0                        273018                   543235             626034   \n",
       "1                        243543                   483382             561709   \n",
       "2                        278274                   529392             610919   \n",
       "3                        300429                   572052             667830   \n",
       "4                        365144                   697324             804438   \n",
       "...                         ...                      ...                ...   \n",
       "1995                     138660                   205602             259781   \n",
       "1996                     129013                   192740             243738   \n",
       "1997                     142740                   204520             260385   \n",
       "1998                     150693                   213564             271864   \n",
       "1999                     170537                   233139             292884   \n",
       "\n",
       "      Baseline                          Day  S/R Ratio  \n",
       "0            1  2025-08-04 00:00:00.000 UTC       1.99  \n",
       "1            1  2025-08-03 00:00:00.000 UTC       1.98  \n",
       "2            1  2025-08-02 00:00:00.000 UTC       1.90  \n",
       "3            1  2025-08-01 00:00:00.000 UTC       1.90  \n",
       "4            1  2025-07-31 00:00:00.000 UTC       1.91  \n",
       "...        ...                          ...        ...  \n",
       "1995         1  2020-02-17 00:00:00.000 UTC       1.48  \n",
       "1996         1  2020-02-16 00:00:00.000 UTC       1.49  \n",
       "1997         1  2020-02-15 00:00:00.000 UTC       1.43  \n",
       "1998         1  2020-02-14 00:00:00.000 UTC       1.42  \n",
       "1999         1  2020-02-13 00:00:00.000 UTC       1.37  \n",
       "\n",
       "[2000 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract the list of rows from the ResultsResponse\n",
    "rows = query_result.get_rows()\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d971ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('out.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66eed3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8956a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064b0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe39e94a",
   "metadata": {},
   "source": [
    "# DEX + Noname Coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f07761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to get OHLCV data for a pool, paginating backward from to_timestamp\n",
    "def get_ohlcv(network, pool_address, from_timestamp, to_timestamp, filename, timeframe='minute', aggregate=1, currency='usd', token='base'):\n",
    "    url = f\"https://api.geckoterminal.com/api/v2/networks/{network}/pools/{pool_address}/ohlcv/{timeframe}\"\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    \n",
    "    # Load existing data\n",
    "    ohlcv_all = []\n",
    "    last_fetched_ts = to_timestamp  # Start from the end\n",
    "    if os.path.exists(filename):\n",
    "        df_existing = pd.read_csv(filename)\n",
    "        ohlcv_all = df_existing.values.tolist()\n",
    "        if ohlcv_all:\n",
    "            ohlcv_all.sort(key=lambda x: x[0])  # Ascending order\n",
    "            last_fetched_ts = int(ohlcv_all[0][0])  # Resume from oldest timestamp\n",
    "            print(f\"Resuming from oldest timestamp: {datetime.fromtimestamp(last_fetched_ts)}\")\n",
    "    \n",
    "    current_before = last_fetched_ts\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            'aggregate': aggregate,\n",
    "            'before_timestamp': current_before,\n",
    "            'limit': 1000,\n",
    "            'currency': currency,\n",
    "            'token': token\n",
    "        }\n",
    "        try:\n",
    "            print(f\"Requesting OHLCV: {url} with before_timestamp={datetime.fromtimestamp(current_before)}\")\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            ohlcv_list = data['data']['attributes']['ohlcv_list']\n",
    "            \n",
    "            if not ohlcv_list:\n",
    "                print(f\"No OHLCV data returned for {network}/{pool_address} before {datetime.fromtimestamp(current_before)}. Likely reached pool creation.\")\n",
    "                break\n",
    "            \n",
    "            # Sort ascending and filter to avoid duplicates\n",
    "            ohlcv_list.sort(key=lambda x: x[0])\n",
    "            new_ohlcv = [candle for candle in ohlcv_list if from_timestamp <= candle[0] < current_before]\n",
    "            \n",
    "            if not new_ohlcv:\n",
    "                print(f\"No new OHLCV data in range for {network}/{pool_address}\")\n",
    "                break\n",
    "            \n",
    "            # Prepend new data (since fetching older data)\n",
    "            ohlcv_all = new_ohlcv + ohlcv_all\n",
    "            save_ohlcv_to_csv(ohlcv_all, filename)\n",
    "            \n",
    "            oldest_ts = new_ohlcv[0][0]\n",
    "            print(f\"Fetched {len(new_ohlcv)} candles, oldest: {datetime.fromtimestamp(oldest_ts)}\")\n",
    "            if oldest_ts <= from_timestamp:\n",
    "                print(f\"Reached start of range: {datetime.fromtimestamp(from_timestamp)}\")\n",
    "                break\n",
    "            current_before = oldest_ts\n",
    "            \n",
    "            time.sleep(2)\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"HTTP error for {network}/{pool_address}: {e}, status code: {e.response.status_code}\")\n",
    "            if e.response.status_code == 429:\n",
    "                print(\"Rate limit exceeded. Waiting 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            elif e.response.status_code == 404:\n",
    "                print(f\"Pool {pool_address} not found. Marking as failed.\")\n",
    "                save_ohlcv_to_csv(ohlcv_all, filename)\n",
    "                return ohlcv_all\n",
    "            elif e.response.status_code == 422:\n",
    "                print(f\"Pool {pool_address} has >2 tokens, not supported. Marking as failed.\")\n",
    "                save_ohlcv_to_csv(ohlcv_all, filename)\n",
    "                return ohlcv_all\n",
    "            save_ohlcv_to_csv(ohlcv_all, filename)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"General error for {network}/{pool_address}: {e}\")\n",
    "            save_ohlcv_to_csv(ohlcv_all, filename)\n",
    "            break\n",
    "    \n",
    "    ohlcv_filtered = [candle for candle in ohlcv_all if from_timestamp <= candle[0] <= to_timestamp]\n",
    "    ohlcv_filtered.sort(key=lambda x: x[0])\n",
    "    return ohlcv_filtered\n",
    "\n",
    "# Helper to save OHLCV to CSV\n",
    "def save_ohlcv_to_csv(ohlcv_data, filename):\n",
    "    if not ohlcv_data:\n",
    "        print(f\"No data to save for {filename}\")\n",
    "        return\n",
    "    df = pd.DataFrame(ohlcv_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume_usd'])\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved to {filename}. Rows: {len(df)}\")\n",
    "\n",
    "# Save/load progress\n",
    "def save_progress(progress_file, key, status, last_ts=None):\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            progress = json.load(f)\n",
    "    else:\n",
    "        progress = {}\n",
    "    \n",
    "    progress[str(key)] = {'status': status, 'last_ts': last_ts}\n",
    "    \n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump(progress, f)\n",
    "    print(f\"Updated progress: {key} -> {status}\")\n",
    "\n",
    "def load_progress(progress_file):\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77dd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking pool: ('solana', 'EXQfMJKTnhHRv3xCzPttk9sZp6MKMdipeDnjKbGJq4fy')\n",
      "Skipping completed: ('solana', 'EXQfMJKTnhHRv3xCzPttk9sZp6MKMdipeDnjKbGJq4fy')\n",
      "Checking pool: ('solana', 'EwUU8oiKTomjekU5eszctx3kneKJnKm5BJpn5sTT4CUf')\n",
      "Skipping completed: ('solana', 'EwUU8oiKTomjekU5eszctx3kneKJnKm5BJpn5sTT4CUf')\n",
      "Checking pool: ('solana', 'BgdLpSBcEny1QZLMQGxW42az4iAb38ijwq5bxyHTs1mV')\n",
      "Skipping completed: ('solana', 'BgdLpSBcEny1QZLMQGxW42az4iAb38ijwq5bxyHTs1mV')\n",
      "Checking pool: ('solana', '7rQd8FhC1rimV3v9edCRZ6RNFsJN1puXM9UmjaURJRNj')\n",
      "Skipping completed: ('solana', '7rQd8FhC1rimV3v9edCRZ6RNFsJN1puXM9UmjaURJRNj')\n",
      "Checking pool: ('solana', 'e5h5bxlranyjfhezvr3r2j6kgdq3fnx8spkssgyqyya8')\n",
      "Fetching OHLCV for ('solana', 'e5h5bxlranyjfhezvr3r2j6kgdq3fnx8spkssgyqyya8') from 2025-05-01 00:00:00 to 2025-08-25 00:00:00\n",
      "Requesting OHLCV: https://api.geckoterminal.com/api/v2/networks/solana/pools/e5h5bxlranyjfhezvr3r2j6kgdq3fnx8spkssgyqyya8/ohlcv/minute with before_timestamp=2025-08-25 00:00:00\n",
      "HTTP error for solana/e5h5bxlranyjfhezvr3r2j6kgdq3fnx8spkssgyqyya8: 404 Client Error: Not Found for url: https://api.geckoterminal.com/api/v2/networks/solana/pools/e5h5bxlranyjfhezvr3r2j6kgdq3fnx8spkssgyqyya8/ohlcv/minute?aggregate=1&before_timestamp=1756069200&limit=1000&currency=usd&token=base, status code: 404\n",
      "Pool e5h5bxlranyjfhezvr3r2j6kgdq3fnx8spkssgyqyya8 not found. Marking as failed.\n",
      "No data to save for solana_e5h5bxlranyjfhezvr3r2j6kgdq3fnx8spkssgyqyya8.csv\n",
      "Updated progress: ('solana', 'e5h5bxlranyjfhezvr3r2j6kgdq3fnx8spkssgyqyya8') -> failed\n",
      "No data saved for ('solana', 'e5h5bxlranyjfhezvr3r2j6kgdq3fnx8spkssgyqyya8'). Marked as failed.\n"
     ]
    }
   ],
   "source": [
    "# Main script\n",
    "pools = [\n",
    "    {'network': 'solana', 'pool_address': 'EXQfMJKTnhHRv3xCzPttk9sZp6MKMdipeDnjKbGJq4fy', 'from_date': '2025-05-01', 'to_date': '2025-08-25'},\n",
    "    {'network': 'solana', 'pool_address': 'EwUU8oiKTomjekU5eszctx3kneKJnKm5BJpn5sTT4CUf', 'from_date': '2025-05-01', 'to_date': '2025-08-25'},\n",
    "    {'network': 'solana', 'pool_address': 'BgdLpSBcEny1QZLMQGxW42az4iAb38ijwq5bxyHTs1mV', 'from_date': '2025-05-01', 'to_date': '2025-08-25'},\n",
    "    {'network': 'solana', 'pool_address': '7rQd8FhC1rimV3v9edCRZ6RNFsJN1puXM9UmjaURJRNj', 'from_date': '2025-05-01', 'to_date': '2025-08-25'}\n",
    "]\n",
    "\n",
    "progress_file = 'progress.json'\n",
    "progress = load_progress(progress_file)\n",
    "\n",
    "default_to = datetime.now()\n",
    "default_from = default_to - timedelta(days=1)\n",
    "\n",
    "for pool in pools:\n",
    "    key = (pool['network'], pool['pool_address'])\n",
    "    key_str = str(key)\n",
    "    filename = f\"{key[0]}_{key[1]}.csv\"\n",
    "    \n",
    "    print(f\"Checking pool: {key}\")\n",
    "    if key_str in progress and progress[key_str]['status'] == 'complete':\n",
    "        print(f\"Skipping completed: {key}\")\n",
    "        continue\n",
    "    \n",
    "    # Parse dates\n",
    "    if 'from_date' in pool and 'to_date' in pool:\n",
    "        from_dt = datetime.strptime(pool['from_date'], '%Y-%m-%d')\n",
    "        to_dt = datetime.strptime(pool['to_date'], '%Y-%m-%d')\n",
    "    else:\n",
    "        from_dt = default_from\n",
    "        to_dt = default_to\n",
    "    \n",
    "    from_ts = int(from_dt.timestamp())\n",
    "    to_ts = int(to_dt.timestamp())\n",
    "    \n",
    "    if key_str in progress and progress[key_str]['status'] == 'partial':\n",
    "        from_ts = max(from_ts, progress[key_str]['last_ts'])\n",
    "        print(f\"Resuming from last_ts: {datetime.fromtimestamp(from_ts)}\")\n",
    "    \n",
    "    print(f\"Fetching OHLCV for {key} from {datetime.fromtimestamp(from_ts)} to {datetime.fromtimestamp(to_ts)}\")\n",
    "    ohlcv = get_ohlcv(pool['network'], pool['pool_address'], from_ts, to_ts, filename)\n",
    "    \n",
    "    if ohlcv:\n",
    "        save_ohlcv_to_csv(ohlcv, filename)\n",
    "        save_progress(progress_file, key, 'complete')\n",
    "    else:\n",
    "        if os.path.exists(filename):\n",
    "            df = pd.read_csv(filename)\n",
    "            if not df.empty:\n",
    "                last_ts = int(pd.to_datetime(df['timestamp']).min().timestamp())\n",
    "                save_progress(progress_file, key, 'partial', last_ts)\n",
    "                print(f\"Partial progress saved for {key}. Restart from {datetime.fromtimestamp(last_ts)}\")\n",
    "        else:\n",
    "            save_progress(progress_file, key, 'failed', to_ts)\n",
    "            print(f\"No data saved for {key}. Marked as failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25d2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
